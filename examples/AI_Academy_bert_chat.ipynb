{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "j5DlMVxzFGlS",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HugginFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2hftlucmDVZ7",
    "colab_type": "code",
    "outputId": "dac476ce-bc8d-497b-895a-7a9b470f03b6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 20 06:32:55 2018       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   30C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_iMMTBKcDtZF",
    "colab_type": "code",
    "outputId": "11fda79e-e84a-40c1-a068-dc11b175c41c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 1073750016 bytes == 0x57e5a000 @  0x7fe9c34d32a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n"
     ]
    }
   ],
   "source": [
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "!pip install -q konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "qKVlFXn2KSNz",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "# !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "# !apt-get update -qq 2>&1 > /dev/null\n",
    "# !apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "# creds = GoogleCredentials.get_application_default()\n",
    "# import getpass\n",
    "# !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "# vcode = getpass.getpass()\n",
    "# !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "rbv3kJucKSVa",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# !mkdir -p drive  # 구글 드라이브 디렉토리 생성\n",
    "# !google-drive-ocamlfuse drive # 생성한 디렉토리에 구글 드라이브 연동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "cF1PR_EOE-Sp",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import unicodedata\n",
    "import six\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from torch.optim import Optimizer\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from argparse import Namespace\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wN6Epjlnp9FR",
    "colab_type": "text"
   },
   "source": [
    "## 1.context finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "zRjuj14gLGK_",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class ContextFinder:\n",
    "\n",
    "    # BASE_PATH : Project path\n",
    "    # vectorize : Initialize TF-IDF matrix\n",
    "    # documents : documents ( contexts in selected title or paragraph )\n",
    "    # X : Generated TF-IDF weights matrix after fitting input documents\n",
    "    # features : a.k.a vocabulary\n",
    "    # tokenizer : Open-Korean-Text for Korean language processing\n",
    "    def __init__(self):\n",
    "        self.BASE_PATH = os.path.dirname(os.path.abspath(__name__))\n",
    "        self.vectorize = self.init_tf_idf_vector()\n",
    "        self.documents = []\n",
    "        self.X = None\n",
    "        self.features = None\n",
    "        self.tokenizer = Okt()\n",
    "\n",
    "    # tokenization\n",
    "    # norm : ㅋㅋㅋㅋㅋ ---> ㅋㅋ\n",
    "    # stem : 들어간다 ---> 들어가다.\n",
    "    def convert_to_lemma(self, text: str) -> list:\n",
    "        return self.tokenizer.morphs(text, norm=True, stem=True)\n",
    "\n",
    "    # for testing, pos tagged tuple list\n",
    "    def check_pos(self, text: str) -> list:\n",
    "        return self.tokenizer.pos(text)\n",
    "\n",
    "\n",
    "    # loading dev data\n",
    "    def load_context_by_title(self, dataset_path):\n",
    "        if dataset_path is None:\n",
    "            dataset_path = 'dev-v1.1.json'\n",
    "        with open(dataset_path) as f:\n",
    "            data = json.load(f)['data']\n",
    "            for article in data:\n",
    "                for paragraph in article.get('paragraphs'):\n",
    "                    self.documents.append(paragraph.get('context'))\n",
    "\n",
    "    # initializing vectorizer object, adding custom tokenizer above (convert_to_lemma)\n",
    "    def init_tf_idf_vector(self):\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        return TfidfVectorizer(\n",
    "            tokenizer=self.convert_to_lemma,\n",
    "            min_df=1,\n",
    "            sublinear_tf=True\n",
    "        )\n",
    "\n",
    "    def generate_tf_idf_vector(self):\n",
    "        self.X = self.vectorize.fit_transform(self.documents)\n",
    "        self.features = self.vectorize.get_feature_names()\n",
    "        # e.g) after fitting 5 sentences and 7 features, matrix X looks like below\n",
    "        # ([[0.        , 0.40824829, 0.81649658, 0.        , 0.        , 0.        , 0.40824829],\n",
    "        # [0.        , 0.40824829, 0.40824829, 0.        , 0.        , 0.        , 0.81649658],\n",
    "        # [0.41680418, 0.        , 0.        , 0.69197025, 0.41680418, 0.41680418, 0.        ],\n",
    "        # [0.76944707, 0.        , 0.        , 0.63871058, 0.        , 0.        , 0.        ],\n",
    "        # [0.        , 0.        , 0.        , 0.8695635 , 0.34918428, 0.34918428, 0.        ]])\n",
    "\n",
    "    def build_model(self, dataset_path=None):\n",
    "        self.load_context_by_title(dataset_path)\n",
    "        self.generate_tf_idf_vector()\n",
    "\n",
    "    def get_ntop_context(self, query: str, n: int) -> str:\n",
    "        if self.X is None or self.features is None:\n",
    "            self.build_model()\n",
    "\n",
    "        # check input query keywords if they are in feature(vocabulary)\n",
    "        keywords = [word for word in ContextFinder.convert_to_lemma(query) if word in self.features]\n",
    "\n",
    "        # get indexes of keywords in X( TF-IDF matrix )\n",
    "        matched_keywords = np.asarray(self.X.toarray())[:, [self.vectorize.vocabulary_.get(i) for i in keywords]]\n",
    "        #       word 1      word 2\n",
    "        # 0     0.000000    0.000000 doc 1\n",
    "        # 1     0.000000    0.000000 doc 2\n",
    "        # 2     0.416804    0.691970 doc 3\n",
    "        # 3     0.769447    0.638711 doc 4\n",
    "        # 4     0.000000    0.869563 doc 5\n",
    "\n",
    "        # sum each words weights document by document and sorting reverse order\n",
    "        scores = matched_keywords.sum(axis=1).argsort()[::-1]\n",
    "        for i in scores[:n]:\n",
    "            if scores[i] > 0:\n",
    "                yield self.documents[i]\n",
    "\n",
    "    def get_ntop_context_by_cosine_similarity(self, query: str, n: int):\n",
    "        from sklearn.metrics.pairwise import linear_kernel\n",
    "        if self.X is None or self.features is None:\n",
    "            self.build_model()\n",
    "        query_vector = self.vectorize.transform([query])\n",
    "\n",
    "        # linear_kernel is dot product between query_vector and all documents vector and transform 1 dim array\n",
    "        cosine_similar = linear_kernel(query_vector, self.X).flatten()\n",
    "        ranked_idx = cosine_similar.argsort()[::-1]\n",
    "        for i in ranked_idx[:n]:\n",
    "            if cosine_similar[i] > 0:\n",
    "                yield self.documents[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Q8Q4c_7Foyn",
    "colab_type": "text"
   },
   "source": [
    "## 2.optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "w1t6v1DIFpaV",
    "colab_type": "code",
    "outputId": "9078c340-96d8-46ad-e0d5-59946533102e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235.0
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ad088cca9486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBERTAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \"\"\"Implements BERT version of Adam algorithm with weight decay fix (and no ).\n\u001b[1;32m     25\u001b[0m     \u001b[0mParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Optimizer' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "def warmup_cosine(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return 0.5 * (1.0 + torch.cos(math.pi * x))\n",
    "\n",
    "def warmup_constant(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return 1.0\n",
    "\n",
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return 1.0 - x\n",
    "\n",
    "SCHEDULES = {\n",
    "    'warmup_cosine':warmup_cosine,\n",
    "    'warmup_constant':warmup_constant,\n",
    "    'warmup_linear':warmup_linear,\n",
    "}\n",
    "\n",
    "\n",
    "class BERTAdam(Optimizer):\n",
    "    \"\"\"Implements BERT version of Adam algorithm with weight decay fix (and no ).\n",
    "    Params:\n",
    "        lr: learning rate\n",
    "        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\n",
    "        t_total: total number of training steps for the learning\n",
    "            rate schedule, -1  means constant learning rate. Default: -1\n",
    "        schedule: schedule to use for the warmup (see above). Default: 'warmup_linear'\n",
    "        b1: Adams b1. Default: 0.9\n",
    "        b2: Adams b2. Default: 0.999\n",
    "        e: Adams epsilon. Default: 1e-6\n",
    "        weight_decay_rate: Weight decay. Default: 0.01\n",
    "        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr, warmup=-1, t_total=-1, schedule='warmup_linear',\n",
    "                 b1=0.9, b2=0.999, e=1e-6, weight_decay_rate=0.01,\n",
    "                 max_grad_norm=1.0):\n",
    "        if not lr >= 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
    "        if schedule not in SCHEDULES:\n",
    "            raise ValueError(\"Invalid schedule parameter: {}\".format(schedule))\n",
    "        if not 0.0 <= warmup < 1.0 and not warmup == -1:\n",
    "            raise ValueError(\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\".format(warmup))\n",
    "        if not 0.0 <= b1 < 1.0:\n",
    "            raise ValueError(\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\".format(b1))\n",
    "        if not 0.0 <= b2 < 1.0:\n",
    "            raise ValueError(\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\".format(b2))\n",
    "        if not e >= 0.0:\n",
    "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(e))\n",
    "        defaults = dict(lr=lr, schedule=schedule, warmup=warmup, t_total=t_total,\n",
    "                        b1=b1, b2=b2, e=e, weight_decay_rate=weight_decay_rate,\n",
    "                        max_grad_norm=max_grad_norm)\n",
    "        super(BERTAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    return [0]\n",
    "                if group['t_total'] != -1:\n",
    "                    schedule_fct = SCHEDULES[group['schedule']]\n",
    "                    lr_scheduled = group['lr'] * schedule_fct(state['step']/group['t_total'], group['warmup'])\n",
    "                else:\n",
    "                    lr_scheduled = group['lr']\n",
    "                lr.append(lr_scheduled)\n",
    "        return lr\n",
    "\n",
    "    def to(self, device):\n",
    "        \"\"\" Move the optimizer state to a specified device\"\"\"\n",
    "        for state in self.state.values():\n",
    "            state['exp_avg'].to(device)\n",
    "            state['exp_avg_sq'].to(device)\n",
    "\n",
    "    def initialize_step(self, initial_step):\n",
    "        \"\"\"Initialize state with a defined step (but we don't have stored averaged).\n",
    "        Arguments:\n",
    "            initial_step (int): Initial step number.\n",
    "        \"\"\"\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                # State initialization\n",
    "                state['step'] = initial_step\n",
    "                # Exponential moving average of gradient values\n",
    "                state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                # Exponential moving average of squared gradient values\n",
    "                state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['next_m'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['next_v'] = torch.zeros_like(p.data)\n",
    "\n",
    "                next_m, next_v = state['next_m'], state['next_v']\n",
    "                beta1, beta2 = group['b1'], group['b2']\n",
    "\n",
    "                # Add grad clipping\n",
    "                if group['max_grad_norm'] > 0:\n",
    "                    clip_grad_norm_(p, group['max_grad_norm'])\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # In-place operations to update the averages at the same time\n",
    "                next_m.mul_(beta1).add_(1 - beta1, grad)\n",
    "                next_v.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                update = next_m / (next_v.sqrt() + group['e'])\n",
    "\n",
    "                # Just adding the square of the weights to the loss function is *not*\n",
    "                # the correct way of using L2 regularization/weight decay with Adam,\n",
    "                # since that will interact with the m and v parameters in strange ways.\n",
    "                #\n",
    "                # Instead we want ot decay the weights in a manner that doesn't interact\n",
    "                # with the m/v parameters. This is equivalent to adding the square\n",
    "                # of the weights to the loss with plain (non-momentum) SGD.\n",
    "                if group['weight_decay_rate'] > 0.0:\n",
    "                    update += group['weight_decay_rate'] * p.data\n",
    "\n",
    "                if group['t_total'] != -1:\n",
    "                    schedule_fct = SCHEDULES[group['schedule']]\n",
    "                    lr_scheduled = group['lr'] * schedule_fct(state['step']/group['t_total'], group['warmup'])\n",
    "                else:\n",
    "                    lr_scheduled = group['lr']\n",
    "\n",
    "                update_with_lr = lr_scheduled * update\n",
    "                p.data.add_(-update_with_lr)\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n",
    "                # bias_correction1 = 1 - beta1 ** state['step']\n",
    "                # bias_correction2 = 1 - beta2 ** state['step']\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQgxzNLJEn_s",
    "colab_type": "text"
   },
   "source": [
    "## 3.tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "n2EKsoFFFSbl",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "\n",
    "\n",
    "def printable_text(text):\n",
    "    \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n",
    "\n",
    "    # These functions want `str` for both Python2 and Python3, but in one case\n",
    "    # it's a Unicode string and in the other it's a byte string.\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, unicode):\n",
    "            return text.encode(\"utf-8\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    index = 0\n",
    "    with open(vocab_file, \"r\") as reader:\n",
    "        while True:\n",
    "            token = convert_to_unicode(reader.readline())\n",
    "            if not token:\n",
    "                break\n",
    "            token = token.strip()\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def convert_tokens_to_ids(vocab, tokens):\n",
    "    \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
    "    ids = []\n",
    "    for token in tokens:\n",
    "        ids.append(vocab[token])\n",
    "    return ids\n",
    "\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "    \"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def _is_whitespace(char):\n",
    "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
    "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
    "    # as whitespace since they are generally considered as such.\n",
    "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat == \"Zs\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_control(char):\n",
    "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
    "    # These are technically control characters but we count them as whitespace\n",
    "    # characters.\n",
    "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return False\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"C\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_punctuation(char):\n",
    "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
    "    cp = ord(char)\n",
    "    # We treat all non-letter/number ASCII as punctuation.\n",
    "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
    "    # Punctuation class but we treat them as punctuation anyways, for\n",
    "    # consistency.\n",
    "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"P\"):\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Gf9ZAwAMgm_",
    "colab_type": "text"
   },
   "source": [
    "### 3.1 FullTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "fCGsDh_yMgwz",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class FullTokenizer(object):\n",
    "    \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_file, do_lower_case=True):\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        for token in self.basic_tokenizer.tokenize(text):\n",
    "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "                split_tokens.append(sub_token)\n",
    "\n",
    "        return split_tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return convert_tokens_to_ids(self.vocab, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dozc7fx3MRYt",
    "colab_type": "text"
   },
   "source": [
    "### 3.2 BasicTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "NFRFCWdaMRgd",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class BasicTokenizer(object):\n",
    "    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
    "\n",
    "    def __init__(self, do_lower_case=True):\n",
    "        \"\"\"Constructs a BasicTokenizer.\n",
    "\n",
    "        Args:\n",
    "          do_lower_case: Whether to lower case the input.\n",
    "        \"\"\"\n",
    "        self.do_lower_case = do_lower_case\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenizes a piece of text.\"\"\"\n",
    "        text = convert_to_unicode(text)\n",
    "        text = self._clean_text(text)\n",
    "        orig_tokens = whitespace_tokenize(text)\n",
    "        split_tokens = []\n",
    "        for token in orig_tokens:\n",
    "            if self.do_lower_case:\n",
    "                token = token.lower()\n",
    "                token = self._run_strip_accents(token)\n",
    "            split_tokens.extend(self._run_split_on_punc(token))\n",
    "\n",
    "        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
    "        return output_tokens\n",
    "\n",
    "    def _run_strip_accents(self, text):\n",
    "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
    "        text = unicodedata.normalize(\"NFD\", text)\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cat = unicodedata.category(char)\n",
    "            if cat == \"Mn\":\n",
    "                continue\n",
    "            output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def _run_split_on_punc(self, text):\n",
    "        \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
    "        chars = list(text)\n",
    "        i = 0\n",
    "        start_new_word = True\n",
    "        output = []\n",
    "        while i < len(chars):\n",
    "            char = chars[i]\n",
    "            if _is_punctuation(char):\n",
    "                output.append([char])\n",
    "                start_new_word = True\n",
    "            else:\n",
    "                if start_new_word:\n",
    "                    output.append([])\n",
    "                start_new_word = False\n",
    "                output[-1].append(char)\n",
    "            i += 1\n",
    "\n",
    "        return [\"\".join(x) for x in output]\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cp = ord(char)\n",
    "            if cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "                continue\n",
    "            if _is_whitespace(char):\n",
    "                output.append(\" \")\n",
    "            else:\n",
    "                output.append(char)\n",
    "        return \"\".join(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86GI1W9tMRsx",
    "colab_type": "text"
   },
   "source": [
    "### 3.3 WordpieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Whh85ituMR9F",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class WordpieceTokenizer(object):\n",
    "    \"\"\"Runs WordPiece tokenization.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=100):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenizes a piece of text into its word pieces.\n",
    "\n",
    "        This uses a greedy longest-match-first algorithm to perform tokenization\n",
    "        using the given vocabulary.\n",
    "\n",
    "        For example:\n",
    "          input = \"unaffable\"\n",
    "          output = [\"un\", \"##aff\", \"##able\"]\n",
    "\n",
    "        Args:\n",
    "          text: A single token or whitespace separated tokens. This should have\n",
    "            already been passed through `BasicTokenizer.\n",
    "\n",
    "        Returns:\n",
    "          A list of wordpiece tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        text = convert_to_unicode(text)\n",
    "\n",
    "        output_tokens = []\n",
    "        for token in whitespace_tokenize(text):\n",
    "            chars = list(token)\n",
    "            if len(chars) > self.max_input_chars_per_word:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "\n",
    "            is_bad = False\n",
    "            start = 0\n",
    "            sub_tokens = []\n",
    "            while start < len(chars):\n",
    "                end = len(chars)\n",
    "                cur_substr = None\n",
    "                while start < end:\n",
    "                    substr = \"\".join(chars[start:end])\n",
    "                    if start > 0:\n",
    "                        substr = \"##\" + substr\n",
    "                    if substr in self.vocab:\n",
    "                        cur_substr = substr\n",
    "                        break\n",
    "                    end -= 1\n",
    "                if cur_substr is None:\n",
    "                    is_bad = True\n",
    "                    break\n",
    "                sub_tokens.append(cur_substr)\n",
    "                start = end\n",
    "\n",
    "            if is_bad:\n",
    "                output_tokens.append(self.unk_token)\n",
    "            else:\n",
    "                output_tokens.extend(sub_tokens)\n",
    "        return output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3TOVWNtGE0z",
    "colab_type": "text"
   },
   "source": [
    "## 4.modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTP-mIZvuxjD",
    "colab_type": "text"
   },
   "source": [
    "### 4.1 gelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "046Plon9uMRY",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"\"\"Implementation of the gelu activation function.\n",
    "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
    "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZ4Ipqi8uzkY",
    "colab_type": "text"
   },
   "source": [
    "### 4.2 Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "UPnMEZ3MuMUa",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class BertConfig(object):\n",
    "    \"\"\"Configuration class to store the configuration of a `BertModel`.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                vocab_size,\n",
    "                hidden_size=768,\n",
    "                num_hidden_layers=12,\n",
    "                num_attention_heads=12,\n",
    "                intermediate_size=3072,\n",
    "                hidden_act=\"gelu\",\n",
    "                hidden_dropout_prob=0.1,\n",
    "                attention_probs_dropout_prob=0.1,\n",
    "                max_position_embeddings=512,\n",
    "                type_vocab_size=16,\n",
    "                initializer_range=0.02):\n",
    "        \"\"\"Constructs BertConfig.\n",
    "\n",
    "        Args:\n",
    "            vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n",
    "            hidden_size: Size of the encoder layers and the pooler layer.\n",
    "            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n",
    "            num_attention_heads: Number of attention heads for each attention layer in\n",
    "                the Transformer encoder.\n",
    "            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
    "                layer in the Transformer encoder.\n",
    "            hidden_act: The non-linear activation function (function or string) in the\n",
    "                encoder and pooler.\n",
    "            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
    "                layers in the embeddings, encoder, and pooler.\n",
    "            attention_probs_dropout_prob: The dropout ratio for the attention\n",
    "                probabilities.\n",
    "            max_position_embeddings: The maximum sequence length that this model might\n",
    "                ever be used with. Typically set this to something large just in case\n",
    "                (e.g., 512 or 1024 or 2048).\n",
    "            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
    "                `BertModel`.\n",
    "            initializer_range: The sttdev of the truncated_normal_initializer for\n",
    "                initializing all weight matrices.\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, json_object):\n",
    "        \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n",
    "        config = BertConfig(vocab_size=None)\n",
    "        for (key, value) in six.iteritems(json_object):\n",
    "            config.__dict__[key] = value\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_json_file(cls, json_file):\n",
    "        \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
    "        with open(json_file, \"r\") as reader:\n",
    "            text = reader.read()\n",
    "        return cls.from_dict(json.loads(text))\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lt_ynQWEu2Uu",
    "colab_type": "text"
   },
   "source": [
    "### 4.3 LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "KhyATpFHuMXn",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class BERTLayerNorm(nn.Module):\n",
    "    def __init__(self, config, variance_epsilon=1e-12):\n",
    "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
    "        \"\"\"\n",
    "        super(BERTLayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(config.hidden_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(config.hidden_size))\n",
    "        self.variance_epsilon = variance_epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.gamma * x + self.beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vL4Xi_icvAt9",
    "colab_type": "text"
   },
   "source": [
    "### 4.4 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "SMFJEkQCuMav",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class BERTEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BERTEmbeddings, self).__init__()\n",
    "        \"\"\"Construct the embedding module from word, position and token_type embeddings.\n",
    "        \"\"\"\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = BERTLayerNorm(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWPXfUR6vEls",
    "colab_type": "text"
   },
   "source": [
    "### 4.5 SelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "b1-oWyIwuMiV",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class BERTSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BERTSelfAttention, self).__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        return context_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNKf8CzXvG9K",
    "colab_type": "text"
   },
   "source": [
    "### 4.6 SelfOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "0t3tHYDhuMlh",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class BERTSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BERTSelfOutput, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = BERTLayerNorm(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1P38JvjvJK6",
    "colab_type": "text"
   },
   "source": [
    "### 4.7 Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "1l1e5x__uMr2",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class BERTAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BERTAttention, self).__init__()\n",
    "        self.self = BERTSelfAttention(config)\n",
    "        self.output = BERTSelfOutput(config)\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask):\n",
    "        self_output = self.self(input_tensor, attention_mask)\n",
    "        attention_output = self.output(self_output, input_tensor)\n",
    "        return attention_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNfszLWJvUuj",
    "colab_type": "text"
   },
   "source": [
    "### 4.8 Intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Gdte3tzzuMvi",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class BERTIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BERTIntermediate, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.intermediate_act_fn = gelu\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wj1c1SNyvVOK",
    "colab_type": "text"
   },
   "source": [
    "### 4.9 Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "4jFgoHnTuMpH",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class BERTOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BERTOutput, self).__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = BERTLayerNorm(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mvcKZINvVjp",
    "colab_type": "text"
   },
   "source": [
    "### 4.10 Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "QH1orn43uMf1",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class BERTLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BERTLayer, self).__init__()\n",
    "        self.attention = BERTAttention(config)\n",
    "        self.intermediate = BERTIntermediate(config)\n",
    "        self.output = BERTOutput(config)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V157G-OwvWIt",
    "colab_type": "text"
   },
   "source": [
    "### 4.11 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "JCKkSAlcujbq",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BERTEncoder, self).__init__()\n",
    "        layer = BERTLayer(config)\n",
    "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])    \n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        all_encoder_layers = []\n",
    "        for layer_module in self.layer:\n",
    "            hidden_states = layer_module(hidden_states, attention_mask)\n",
    "            all_encoder_layers.append(hidden_states)\n",
    "        return all_encoder_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbgCD-IMvWoF",
    "colab_type": "text"
   },
   "source": [
    "### 4.12 Pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "F6jABaSMuMdv",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class BERTPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BERTPooler, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3TwHE0YvXCp",
    "colab_type": "text"
   },
   "source": [
    "### 4.13 Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "QPY-GIOaFS4U",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    \"\"\"BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
    "\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n",
    "\n",
    "    config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n",
    "        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
    "\n",
    "    model = modeling.BertModel(config=config)\n",
    "    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, config: BertConfig):\n",
    "        \"\"\"Constructor for BertModel.\n",
    "\n",
    "        Args:\n",
    "            config: `BertConfig` instance.\n",
    "        \"\"\"\n",
    "        super(BertModel, self).__init__()\n",
    "        self.embeddings = BERTEmbeddings(config)\n",
    "        self.encoder = BERTEncoder(config)\n",
    "        self.pooler = BERTPooler(config)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        # We create a 3D attention mask from a 2D tensor mask.\n",
    "        # Sizes are [batch_size, 1, 1, from_seq_length]\n",
    "        # So we can broadcast to [batch_size, num_heads, to_seq_length, from_seq_length]\n",
    "        # this attention mask is more simple than the triangular masking of causal attention\n",
    "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        extended_attention_mask = extended_attention_mask.float()\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "        all_encoder_layers = self.encoder(embedding_output, extended_attention_mask)\n",
    "        sequence_output = all_encoder_layers[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        return all_encoder_layers, pooled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiEAp81zt5NL",
    "colab_type": "text"
   },
   "source": [
    "### 4.14 BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "CqNXRN7Dt309",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class BertForQuestionAnswering(nn.Module):\n",
    "    \"\"\"BERT model for Question Answering (span extraction).\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the sequence output that computes start_logits and end_logits\n",
    "\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n",
    "\n",
    "    config = BertConfig(vocab_size=32000, hidden_size=512,\n",
    "        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
    "\n",
    "    model = BertForQuestionAnswering(config)\n",
    "    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertForQuestionAnswering, self).__init__()\n",
    "        self.bert = BertModel(config)\n",
    "        # TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version\n",
    "        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "        def init_weights(module):\n",
    "            if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "                # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "                # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "                module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "            elif isinstance(module, BERTLayerNorm):\n",
    "                module.beta.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "                module.gamma.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "            if isinstance(module, nn.Linear):\n",
    "                module.bias.data.zero_()\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, start_positions=None, end_positions=None):\n",
    "        all_encoder_layers, _ = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "        sequence_output = all_encoder_layers[-1]\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension - if not this is a no-op\n",
    "            start_positions = start_positions.squeeze(-1)\n",
    "            end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "            return total_loss, (start_logits, end_logits)\n",
    "        else:\n",
    "            return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2h0fVJ6GKfI",
    "colab_type": "text"
   },
   "source": [
    "## 5.extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Z_wjmRijGIxa",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "\n",
    "    def __init__(self, unique_id, text_a, text_b):\n",
    "        self.unique_id = unique_id\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids):\n",
    "        self.unique_id = unique_id\n",
    "        self.tokens = tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.input_type_ids = input_type_ids\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "\n",
    "def read_examples(input_file):\n",
    "    \"\"\"Read a list of `InputExample`s from an input file.\"\"\"\n",
    "    examples = []\n",
    "    unique_id = 0\n",
    "    with open(input_file, \"r\") as reader:\n",
    "        while True:\n",
    "            line = tokenization.convert_to_unicode(reader.readline())\n",
    "            if not line:\n",
    "                break\n",
    "            line = line.strip()\n",
    "            text_a = None\n",
    "            text_b = None\n",
    "            m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n",
    "            if m is None:\n",
    "                text_a = line\n",
    "            else:\n",
    "                text_a = m.group(1)\n",
    "                text_b = m.group(2)\n",
    "            examples.append(\n",
    "                InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b))\n",
    "            unique_id += 1\n",
    "    return examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-TPYBk3tcCK",
    "colab_type": "text"
   },
   "source": [
    "### 5.1convert_examples_to_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "0SsBwBYQtbxJ",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
    "                                 doc_stride, max_query_length, is_training):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    unique_id = 1000000000\n",
    "\n",
    "    features = []\n",
    "    for (example_index, example) in enumerate(examples):\n",
    "        query_tokens = tokenizer.tokenize(example.question_text)\n",
    "\n",
    "        if len(query_tokens) > max_query_length:\n",
    "            query_tokens = query_tokens[0:max_query_length]\n",
    "\n",
    "        tok_to_orig_index = []\n",
    "        orig_to_tok_index = []\n",
    "        all_doc_tokens = []\n",
    "        for (i, token) in enumerate(example.doc_tokens):\n",
    "            orig_to_tok_index.append(len(all_doc_tokens))\n",
    "            sub_tokens = tokenizer.tokenize(token)\n",
    "            for sub_token in sub_tokens:\n",
    "                tok_to_orig_index.append(i)\n",
    "                all_doc_tokens.append(sub_token)\n",
    "\n",
    "        tok_start_position = None\n",
    "        tok_end_position = None\n",
    "        if is_training:\n",
    "            tok_start_position = orig_to_tok_index[example.start_position]\n",
    "            if example.end_position < len(example.doc_tokens) - 1:\n",
    "                tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
    "            else:\n",
    "                tok_end_position = len(all_doc_tokens) - 1\n",
    "            (tok_start_position, tok_end_position) = _improve_answer_span(\n",
    "                all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n",
    "                example.orig_answer_text)\n",
    "            \n",
    "            \n",
    "        # The -3 accounts for [CLS], [SEP] and [SEP]\n",
    "        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
    "        \n",
    "        # We can have documents that are longer than the maximum sequence length.\n",
    "        # To deal with this we do a sliding window approach, where we take chunks\n",
    "        # of the up to our max length with a stride of `doc_stride`.\n",
    "        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"DocSpan\", [\"start\", \"length\"])\n",
    "        doc_spans = []\n",
    "        start_offset = 0\n",
    "        while start_offset < len(all_doc_tokens):\n",
    "            length = len(all_doc_tokens) - start_offset\n",
    "            if length > max_tokens_for_doc:\n",
    "                length = max_tokens_for_doc\n",
    "            doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "            if start_offset + length == len(all_doc_tokens):\n",
    "                break\n",
    "            start_offset += min(length, doc_stride)\n",
    "\n",
    "        for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
    "            tokens = []\n",
    "            token_to_orig_map = {}\n",
    "            token_is_max_context = {}\n",
    "            segment_ids = []\n",
    "            tokens.append(\"[CLS]\")\n",
    "            segment_ids.append(0)\n",
    "            for token in query_tokens:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(0)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(0)\n",
    "\n",
    "            for i in range(doc_span.length):\n",
    "                split_token_index = doc_span.start + i\n",
    "                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
    "\n",
    "                is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
    "                                                       split_token_index)\n",
    "                token_is_max_context[len(tokens)] = is_max_context\n",
    "                tokens.append(all_doc_tokens[split_token_index])\n",
    "                segment_ids.append(1)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "            # tokens are attended to.\n",
    "            input_mask = [1] * len(input_ids)\n",
    "\n",
    "            # Zero-pad up to the sequence length.\n",
    "            while len(input_ids) < max_seq_length:\n",
    "                input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "\n",
    "            assert len(input_ids) == max_seq_length\n",
    "            assert len(input_mask) == max_seq_length\n",
    "            assert len(segment_ids) == max_seq_length\n",
    "\n",
    "            start_position = None\n",
    "            end_position = None\n",
    "            if is_training:\n",
    "                # For training, if our document chunk does not contain an annotation\n",
    "                # we throw it out, since there is nothing to predict.\n",
    "                doc_start = doc_span.start\n",
    "                doc_end = doc_span.start + doc_span.length - 1\n",
    "                if (example.start_position < doc_start or\n",
    "                        example.end_position < doc_start or\n",
    "                        example.start_position > doc_end or example.end_position > doc_end):\n",
    "                    continue\n",
    "\n",
    "                doc_offset = len(query_tokens) + 2\n",
    "                start_position = tok_start_position - doc_start + doc_offset\n",
    "                end_position = tok_end_position - doc_start + doc_offset\n",
    "\n",
    "            if example_index < 20:\n",
    "                logger.info(\"*** Example ***\")\n",
    "                logger.info(\"unique_id: %s\" % (unique_id))\n",
    "                logger.info(\"example_index: %s\" % (example_index))\n",
    "                logger.info(\"doc_span_index: %s\" % (doc_span_index))\n",
    "                logger.info(\"tokens: %s\" % \" \".join(tokens))\n",
    "                logger.info(\"token_to_orig_map: %s\" % \" \".join([\n",
    "                    \"%d:%d\" % (x, y) for (x, y) in token_to_orig_map.items()]))\n",
    "                logger.info(\"token_is_max_context: %s\" % \" \".join([\n",
    "                    \"%d:%s\" % (x, y) for (x, y) in token_is_max_context.items()\n",
    "                ]))\n",
    "                logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "                logger.info(\n",
    "                    \"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "                logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "                if is_training:\n",
    "                    answer_text = \" \".join(tokens[start_position:(end_position + 1)])\n",
    "                    logger.info(\"start_position: %d\" % (start_position))\n",
    "                    logger.info(\"end_position: %d\" % (end_position))\n",
    "                    logger.info(\n",
    "                        \"answer: %s\" % (answer_text))\n",
    "\n",
    "            features.append(\n",
    "                InputFeatures(\n",
    "                    unique_id=unique_id,\n",
    "                    example_index=example_index,\n",
    "                    doc_span_index=doc_span_index,\n",
    "                    tokens=tokens,\n",
    "                    token_to_orig_map=token_to_orig_map,\n",
    "                    token_is_max_context=token_is_max_context,\n",
    "                    input_ids=input_ids,\n",
    "                    input_mask=input_mask,\n",
    "                    segment_ids=segment_ids,\n",
    "                    start_position=start_position,\n",
    "                    end_position=end_position))\n",
    "            unique_id += 1\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaflPCLNGKpY",
    "colab_type": "text"
   },
   "source": [
    "## 6.run_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "i_qmoK19GtUY",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class SquadExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 qas_id,\n",
    "                 question_text,\n",
    "                 doc_tokens,\n",
    "                 orig_answer_text=None,\n",
    "                 start_position=None,\n",
    "                 end_position=None):\n",
    "        self.qas_id = qas_id\n",
    "        self.question_text = question_text\n",
    "        self.doc_tokens = doc_tokens\n",
    "        self.orig_answer_text = orig_answer_text\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = \"\"\n",
    "        s += \"qas_id: %s\" % (printable_text(self.qas_id))\n",
    "        s += \", question_text: %s\" % (\n",
    "            printable_text(self.question_text))\n",
    "        s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n",
    "        if self.start_position:\n",
    "            s += \", start_position: %d\" % (self.start_position)\n",
    "        if self.start_position:\n",
    "            s += \", end_position: %d\" % (self.end_position)\n",
    "        return s\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 unique_id,\n",
    "                 example_index,\n",
    "                 doc_span_index,\n",
    "                 tokens,\n",
    "                 token_to_orig_map,\n",
    "                 token_is_max_context,\n",
    "                 input_ids,\n",
    "                 input_mask,\n",
    "                 segment_ids,\n",
    "                 start_position=None,\n",
    "                 end_position=None):\n",
    "        self.unique_id = unique_id\n",
    "        self.example_index = example_index\n",
    "        self.doc_span_index = doc_span_index\n",
    "        self.tokens = tokens\n",
    "        self.token_to_orig_map = token_to_orig_map\n",
    "        self.token_is_max_context = token_is_max_context\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position\n",
    "\n",
    "\n",
    "def read_squad_examples(input_file, is_training):\n",
    "    \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n",
    "    with open(input_file, \"r\") as reader:\n",
    "        input_data = json.load(reader)[\"data\"]\n",
    "\n",
    "    def is_whitespace(c):\n",
    "        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    examples = []\n",
    "    for entry in input_data:\n",
    "        for paragraph in entry[\"paragraphs\"]:\n",
    "            paragraph_text = paragraph[\"context\"]\n",
    "            doc_tokens = []\n",
    "            char_to_word_offset = []\n",
    "            prev_is_whitespace = True\n",
    "            for c in paragraph_text:\n",
    "                if is_whitespace(c):\n",
    "                    prev_is_whitespace = True\n",
    "                else:\n",
    "                    if prev_is_whitespace:\n",
    "                        doc_tokens.append(c)\n",
    "                    else:\n",
    "                        doc_tokens[-1] += c\n",
    "                    prev_is_whitespace = False\n",
    "                char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                qas_id = qa[\"id\"]\n",
    "                question_text = qa[\"question\"]\n",
    "                start_position = None\n",
    "                end_position = None\n",
    "                orig_answer_text = None\n",
    "                if is_training:\n",
    "                    if len(qa[\"answers\"]) != 1:\n",
    "                        raise ValueError(\n",
    "                            \"For training, each question should have exactly 1 answer.\")\n",
    "                    answer = qa[\"answers\"][0]\n",
    "                    orig_answer_text = answer[\"text\"]\n",
    "                    answer_offset = answer[\"answer_start\"]\n",
    "                    answer_length = len(orig_answer_text)\n",
    "                    start_position = char_to_word_offset[answer_offset]\n",
    "                    end_position = char_to_word_offset[answer_offset + answer_length - 1]\n",
    "                    # Only add answers where the text can be exactly recovered from the\n",
    "                    # document. If this CAN'T happen it's likely due to weird Unicode\n",
    "                    # stuff so we will just skip the example.\n",
    "                    #\n",
    "                    # Note that this means for training mode, every example is NOT\n",
    "                    # guaranteed to be preserved.\n",
    "                    actual_text = \" \".join(doc_tokens[start_position:(end_position + 1)])\n",
    "                    cleaned_answer_text = \" \".join(\n",
    "                        whitespace_tokenize(orig_answer_text))\n",
    "                    if actual_text.find(cleaned_answer_text) == -1:\n",
    "                        logger.warning(\"Could not find answer: '%s' vs. '%s'\",\n",
    "                                           actual_text, cleaned_answer_text)\n",
    "                        continue\n",
    "\n",
    "                example = SquadExample(\n",
    "                    qas_id=qas_id,\n",
    "                    question_text=question_text,\n",
    "                    doc_tokens=doc_tokens,\n",
    "                    orig_answer_text=orig_answer_text,\n",
    "                    start_position=start_position,\n",
    "                    end_position=end_position)\n",
    "                examples.append(example)\n",
    "    return examples\n",
    "\n",
    "\n",
    "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
    "                                 doc_stride, max_query_length, is_training):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    unique_id = 1000000000\n",
    "\n",
    "    features = []\n",
    "    for (example_index, example) in enumerate(examples):\n",
    "        query_tokens = tokenizer.tokenize(example.question_text)\n",
    "\n",
    "        if len(query_tokens) > max_query_length:\n",
    "            query_tokens = query_tokens[0:max_query_length]\n",
    "\n",
    "        tok_to_orig_index = []\n",
    "        orig_to_tok_index = []\n",
    "        all_doc_tokens = []\n",
    "        for (i, token) in enumerate(example.doc_tokens):\n",
    "            orig_to_tok_index.append(len(all_doc_tokens))\n",
    "            sub_tokens = tokenizer.tokenize(token)\n",
    "            for sub_token in sub_tokens:\n",
    "                tok_to_orig_index.append(i)\n",
    "                all_doc_tokens.append(sub_token)\n",
    "\n",
    "        tok_start_position = None\n",
    "        tok_end_position = None\n",
    "        if is_training:\n",
    "            tok_start_position = orig_to_tok_index[example.start_position]\n",
    "            if example.end_position < len(example.doc_tokens) - 1:\n",
    "                tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
    "            else:\n",
    "                tok_end_position = len(all_doc_tokens) - 1\n",
    "            (tok_start_position, tok_end_position) = _improve_answer_span(\n",
    "                all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n",
    "                example.orig_answer_text)\n",
    "\n",
    "        # The -3 accounts for [CLS], [SEP] and [SEP]\n",
    "        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
    "\n",
    "        # We can have documents that are longer than the maximum sequence length.\n",
    "        # To deal with this we do a sliding window approach, where we take chunks\n",
    "        # of the up to our max length with a stride of `doc_stride`.\n",
    "        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"DocSpan\", [\"start\", \"length\"])\n",
    "        doc_spans = []\n",
    "        start_offset = 0\n",
    "        while start_offset < len(all_doc_tokens):\n",
    "            length = len(all_doc_tokens) - start_offset\n",
    "            if length > max_tokens_for_doc:\n",
    "                length = max_tokens_for_doc\n",
    "            doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "            if start_offset + length == len(all_doc_tokens):\n",
    "                break\n",
    "            start_offset += min(length, doc_stride)\n",
    "\n",
    "        for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
    "            tokens = []\n",
    "            token_to_orig_map = {}\n",
    "            token_is_max_context = {}\n",
    "            segment_ids = []\n",
    "            tokens.append(\"[CLS]\")\n",
    "            segment_ids.append(0)\n",
    "            for token in query_tokens:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(0)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(0)\n",
    "\n",
    "            for i in range(doc_span.length):\n",
    "                split_token_index = doc_span.start + i\n",
    "                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
    "\n",
    "                is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
    "                                                       split_token_index)\n",
    "                token_is_max_context[len(tokens)] = is_max_context\n",
    "                tokens.append(all_doc_tokens[split_token_index])\n",
    "                segment_ids.append(1)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "            # tokens are attended to.\n",
    "            input_mask = [1] * len(input_ids)\n",
    "\n",
    "            # Zero-pad up to the sequence length.\n",
    "            while len(input_ids) < max_seq_length:\n",
    "                input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "\n",
    "            assert len(input_ids) == max_seq_length\n",
    "            assert len(input_mask) == max_seq_length\n",
    "            assert len(segment_ids) == max_seq_length\n",
    "\n",
    "            start_position = None\n",
    "            end_position = None\n",
    "            if is_training:\n",
    "                # For training, if our document chunk does not contain an annotation\n",
    "                # we throw it out, since there is nothing to predict.\n",
    "                doc_start = doc_span.start\n",
    "                doc_end = doc_span.start + doc_span.length - 1\n",
    "                if (example.start_position < doc_start or\n",
    "                        example.end_position < doc_start or\n",
    "                        example.start_position > doc_end or example.end_position > doc_end):\n",
    "                    continue\n",
    "\n",
    "                doc_offset = len(query_tokens) + 2\n",
    "                start_position = tok_start_position - doc_start + doc_offset\n",
    "                end_position = tok_end_position - doc_start + doc_offset\n",
    "\n",
    "            if example_index < 20:\n",
    "                logger.info(\"*** Example ***\")\n",
    "                logger.info(\"unique_id: %s\" % (unique_id))\n",
    "                logger.info(\"example_index: %s\" % (example_index))\n",
    "                logger.info(\"doc_span_index: %s\" % (doc_span_index))\n",
    "                logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [printable_text(x) for x in tokens]))\n",
    "                logger.info(\"token_to_orig_map: %s\" % \" \".join(\n",
    "                    [\"%d:%d\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n",
    "                logger.info(\"token_is_max_context: %s\" % \" \".join([\n",
    "                    \"%d:%s\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n",
    "                ]))\n",
    "                logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "                logger.info(\n",
    "                    \"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "                logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "                if is_training:\n",
    "                    answer_text = \" \".join(tokens[start_position:(end_position + 1)])\n",
    "                    logger.info(\"start_position: %d\" % (start_position))\n",
    "                    logger.info(\"end_position: %d\" % (end_position))\n",
    "                    logger.info(\n",
    "                        \"answer: %s\" % (printable_text(answer_text)))\n",
    "\n",
    "            features.append(\n",
    "                InputFeatures(\n",
    "                    unique_id=unique_id,\n",
    "                    example_index=example_index,\n",
    "                    doc_span_index=doc_span_index,\n",
    "                    tokens=tokens,\n",
    "                    token_to_orig_map=token_to_orig_map,\n",
    "                    token_is_max_context=token_is_max_context,\n",
    "                    input_ids=input_ids,\n",
    "                    input_mask=input_mask,\n",
    "                    segment_ids=segment_ids,\n",
    "                    start_position=start_position,\n",
    "                    end_position=end_position))\n",
    "            unique_id += 1\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n",
    "                         orig_answer_text):\n",
    "    \"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"\n",
    "\n",
    "    # The SQuAD annotations are character based. We first project them to\n",
    "    # whitespace-tokenized words. But then after WordPiece tokenization, we can\n",
    "    # often find a \"better match\". For example:\n",
    "    #\n",
    "    #   Question: What year was John Smith born?\n",
    "    #   Context: The leader was John Smith (1895-1943).\n",
    "    #   Answer: 1895\n",
    "    #\n",
    "    # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n",
    "    # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n",
    "    # the exact answer, 1895.\n",
    "    #\n",
    "    # However, this is not always possible. Consider the following:\n",
    "    #\n",
    "    #   Question: What country is the top exporter of electornics?\n",
    "    #   Context: The Japanese electronics industry is the lagest in the world.\n",
    "    #   Answer: Japan\n",
    "    #\n",
    "    # In this case, the annotator chose \"Japan\" as a character sub-span of\n",
    "    # the word \"Japanese\". Since our WordPiece tokenizer does not split\n",
    "    # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n",
    "    # in SQuAD, but does happen.\n",
    "    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n",
    "\n",
    "    for new_start in range(input_start, input_end + 1):\n",
    "        for new_end in range(input_end, new_start - 1, -1):\n",
    "            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n",
    "            if text_span == tok_answer_text:\n",
    "                return (new_start, new_end)\n",
    "\n",
    "    return (input_start, input_end)\n",
    "\n",
    "\n",
    "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
    "    \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
    "\n",
    "    # Because of the sliding window approach taken to scoring documents, a single\n",
    "    # token can appear in multiple documents. E.g.\n",
    "    #  Doc: the man went to the store and bought a gallon of milk\n",
    "    #  Span A: the man went to the\n",
    "    #  Span B: to the store and bought\n",
    "    #  Span C: and bought a gallon of\n",
    "    #  ...\n",
    "    #\n",
    "    # Now the word 'bought' will have two scores from spans B and C. We only\n",
    "    # want to consider the score with \"maximum context\", which we define as\n",
    "    # the *minimum* of its left and right context (the *sum* of left and\n",
    "    # right context will always be the same, of course).\n",
    "    #\n",
    "    # In the example the maximum context for 'bought' would be span C since\n",
    "    # it has 1 left context and 3 right context, while span B has 4 left context\n",
    "    # and 0 right context.\n",
    "    best_score = None\n",
    "    best_span_index = None\n",
    "    for (span_index, doc_span) in enumerate(doc_spans):\n",
    "        end = doc_span.start + doc_span.length - 1\n",
    "        if position < doc_span.start:\n",
    "            continue\n",
    "        if position > end:\n",
    "            continue\n",
    "        num_left_context = position - doc_span.start\n",
    "        num_right_context = end - position\n",
    "        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
    "        if best_score is None or score > best_score:\n",
    "            best_score = score\n",
    "            best_span_index = span_index\n",
    "\n",
    "    return cur_span_index == best_span_index\n",
    "\n",
    "\n",
    "\n",
    "RawResult = collections.namedtuple(\"RawResult\",\n",
    "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])\n",
    "\n",
    "\n",
    "def write_predictions(all_examples, all_features, all_results, n_best_size,\n",
    "                      max_answer_length, do_lower_case, output_prediction_file,\n",
    "                      output_nbest_file, verbose_logging):\n",
    "    \"\"\"Write final predictions to the json file.\"\"\"\n",
    "    logger.info(\"Writing predictions to: %s\" % (output_prediction_file))\n",
    "    logger.info(\"Writing nbest to: %s\" % (output_nbest_file))\n",
    "\n",
    "    example_index_to_features = collections.defaultdict(list)\n",
    "    for feature in all_features:\n",
    "        example_index_to_features[feature.example_index].append(feature)\n",
    "\n",
    "    unique_id_to_result = {}\n",
    "    for result in all_results:\n",
    "        unique_id_to_result[result.unique_id] = result\n",
    "\n",
    "    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"PrelimPrediction\",\n",
    "        [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n",
    "\n",
    "    all_predictions = collections.OrderedDict()\n",
    "    all_nbest_json = collections.OrderedDict()\n",
    "    for (example_index, example) in enumerate(all_examples):\n",
    "        features = example_index_to_features[example_index]\n",
    "\n",
    "        prelim_predictions = []\n",
    "        for (feature_index, feature) in enumerate(features):\n",
    "            result = unique_id_to_result[feature.unique_id]\n",
    "\n",
    "            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
    "            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # We could hypothetically create invalid predictions, e.g., predict\n",
    "                    # that the start of the span is in the question. We throw out all\n",
    "                    # invalid predictions.\n",
    "                    if start_index >= len(feature.tokens):\n",
    "                        continue\n",
    "                    if end_index >= len(feature.tokens):\n",
    "                        continue\n",
    "                    if start_index not in feature.token_to_orig_map:\n",
    "                        continue\n",
    "                    if end_index not in feature.token_to_orig_map:\n",
    "                        continue\n",
    "                    if not feature.token_is_max_context.get(start_index, False):\n",
    "                        continue\n",
    "                    if end_index < start_index:\n",
    "                        continue\n",
    "                    length = end_index - start_index + 1\n",
    "                    if length > max_answer_length:\n",
    "                        continue\n",
    "                    prelim_predictions.append(\n",
    "                        _PrelimPrediction(\n",
    "                            feature_index=feature_index,\n",
    "                            start_index=start_index,\n",
    "                            end_index=end_index,\n",
    "                            start_logit=result.start_logits[start_index],\n",
    "                            end_logit=result.end_logits[end_index]))\n",
    "\n",
    "        prelim_predictions = sorted(\n",
    "            prelim_predictions,\n",
    "            key=lambda x: (x.start_logit + x.end_logit),\n",
    "            reverse=True)\n",
    "\n",
    "        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\n",
    "\n",
    "        seen_predictions = {}\n",
    "        nbest = []\n",
    "        for pred in prelim_predictions:\n",
    "            if len(nbest) >= n_best_size:\n",
    "                break\n",
    "            feature = features[pred.feature_index]\n",
    "\n",
    "            tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n",
    "            orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
    "            orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
    "            orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
    "            tok_text = \" \".join(tok_tokens)\n",
    "\n",
    "            # De-tokenize WordPieces that have been split off.\n",
    "            tok_text = tok_text.replace(\" ##\", \"\")\n",
    "            tok_text = tok_text.replace(\"##\", \"\")\n",
    "\n",
    "            # Clean whitespace\n",
    "            tok_text = tok_text.strip()\n",
    "            tok_text = \" \".join(tok_text.split())\n",
    "            orig_text = \" \".join(orig_tokens)\n",
    "\n",
    "            final_text = get_final_text(tok_text, orig_text, do_lower_case, verbose_logging)\n",
    "            if final_text in seen_predictions:\n",
    "                continue\n",
    "\n",
    "            seen_predictions[final_text] = True\n",
    "            nbest.append(\n",
    "                _NbestPrediction(\n",
    "                    text=final_text,\n",
    "                    start_logit=pred.start_logit,\n",
    "                    end_logit=pred.end_logit))\n",
    "\n",
    "        # In very rare edge cases we could have no valid predictions. So we\n",
    "        # just create a nonce prediction in this case to avoid failure.\n",
    "        if not nbest:\n",
    "            nbest.append(\n",
    "                _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
    "\n",
    "        assert len(nbest) >= 1\n",
    "\n",
    "        total_scores = []\n",
    "        for entry in nbest:\n",
    "            total_scores.append(entry.start_logit + entry.end_logit)\n",
    "\n",
    "        probs = _compute_softmax(total_scores)\n",
    "\n",
    "        nbest_json = []\n",
    "        for (i, entry) in enumerate(nbest):\n",
    "            output = collections.OrderedDict()\n",
    "            output[\"text\"] = entry.text\n",
    "            output[\"probability\"] = probs[i]\n",
    "            output[\"start_logit\"] = entry.start_logit\n",
    "            output[\"end_logit\"] = entry.end_logit\n",
    "            nbest_json.append(output)\n",
    "\n",
    "        assert len(nbest_json) >= 1\n",
    "\n",
    "        all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n",
    "        all_nbest_json[example.qas_id] = nbest_json\n",
    "\n",
    "    with open(output_prediction_file, \"w\") as writer:\n",
    "        writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
    "\n",
    "    with open(output_nbest_file, \"w\") as writer:\n",
    "        writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
    "\n",
    "def _get_best_indexes(logits, n_best_size):\n",
    "    \"\"\"Get the n-best logits from a list.\"\"\"\n",
    "    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    best_indexes = []\n",
    "    for i in range(len(index_and_score)):\n",
    "        if i >= n_best_size:\n",
    "            break\n",
    "        best_indexes.append(index_and_score[i][0])\n",
    "    return best_indexes\n",
    "\n",
    "\n",
    "def _compute_softmax(scores):\n",
    "    \"\"\"Compute softmax probability over raw logits.\"\"\"\n",
    "    if not scores:\n",
    "        return []\n",
    "\n",
    "    max_score = None\n",
    "    for score in scores:\n",
    "        if max_score is None or score > max_score:\n",
    "            max_score = score\n",
    "\n",
    "    exp_scores = []\n",
    "    total_sum = 0.0\n",
    "    for score in scores:\n",
    "        x = math.exp(score - max_score)\n",
    "        exp_scores.append(x)\n",
    "        total_sum += x\n",
    "\n",
    "    probs = []\n",
    "    for score in exp_scores:\n",
    "        probs.append(score / total_sum)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08mLR83Pv_bZ",
    "colab_type": "text"
   },
   "source": [
    "### 6.1 chatbot_predition_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "ihFLNdX8v_Ok",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def chatbot_prediction_answer(all_examples, all_features, all_results, n_best_size,\n",
    "                      max_answer_length, do_lower_case):\n",
    "\n",
    "    example_index_to_features = collections.defaultdict(list)\n",
    "    for feature in all_features:\n",
    "        example_index_to_features[feature.example_index].append(feature)\n",
    "\n",
    "    unique_id_to_result = {}\n",
    "    for result in all_results:\n",
    "        unique_id_to_result[result.unique_id] = result\n",
    "\n",
    "    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"PrelimPrediction\",\n",
    "        [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n",
    "\n",
    "    all_predictions = collections.OrderedDict()\n",
    "    all_nbest_json = collections.OrderedDict()\n",
    "    for (example_index, example) in enumerate(all_examples):\n",
    "        features = example_index_to_features[example_index]\n",
    "\n",
    "        prelim_predictions = []\n",
    "        for (feature_index, feature) in enumerate(features):\n",
    "            result = unique_id_to_result[feature.unique_id]\n",
    "\n",
    "            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
    "            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # We could hypothetically create invalid predictions, e.g., predict\n",
    "                    # that the start of the span is in the question. We throw out all\n",
    "                    # invalid predictions.\n",
    "                    if start_index >= len(feature.tokens):\n",
    "                        continue\n",
    "                    if end_index >= len(feature.tokens):\n",
    "                        continue\n",
    "                    if start_index not in feature.token_to_orig_map:\n",
    "                        continue\n",
    "                    if end_index not in feature.token_to_orig_map:\n",
    "                        continue\n",
    "                    if not feature.token_is_max_context.get(start_index, False):\n",
    "                        continue\n",
    "                    if end_index < start_index:\n",
    "                        continue\n",
    "                    length = end_index - start_index + 1\n",
    "                    if length > max_answer_length:\n",
    "                        continue\n",
    "                    prelim_predictions.append(\n",
    "                        _PrelimPrediction(\n",
    "                            feature_index=feature_index,\n",
    "                            start_index=start_index,\n",
    "                            end_index=end_index,\n",
    "                            start_logit=result.start_logits[start_index],\n",
    "                            end_logit=result.end_logits[end_index]))\n",
    "\n",
    "        prelim_predictions = sorted(\n",
    "            prelim_predictions,\n",
    "            key=lambda x: (x.start_logit + x.end_logit),\n",
    "            reverse=True)\n",
    "\n",
    "        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\n",
    "\n",
    "        seen_predictions = {}\n",
    "        nbest = []\n",
    "        for pred in prelim_predictions:\n",
    "            if len(nbest) >= n_best_size:\n",
    "                break\n",
    "            feature = features[pred.feature_index]\n",
    "\n",
    "            tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n",
    "            orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
    "            orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
    "            orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
    "            tok_text = \" \".join(tok_tokens)\n",
    "\n",
    "            # De-tokenize WordPieces that have been split off.\n",
    "            tok_text = tok_text.replace(\" ##\", \"\")\n",
    "            tok_text = tok_text.replace(\"##\", \"\")\n",
    "\n",
    "            # Clean whitespace\n",
    "            tok_text = tok_text.strip()\n",
    "            tok_text = \" \".join(tok_text.split())\n",
    "            orig_text = \" \".join(orig_tokens)\n",
    "\n",
    "            final_text = get_final_text(tok_text, orig_text, do_lower_case, verbose_logging=False)\n",
    "            if final_text in seen_predictions:\n",
    "                continue\n",
    "\n",
    "            seen_predictions[final_text] = True\n",
    "            nbest.append(\n",
    "                _NbestPrediction(\n",
    "                    text=final_text,\n",
    "                    start_logit=pred.start_logit,\n",
    "                    end_logit=pred.end_logit))\n",
    "\n",
    "        # In very rare edge cases we could have no valid predictions. So we\n",
    "        # just create a nonce prediction in this case to avoid failure.\n",
    "        if not nbest:\n",
    "            nbest.append(\n",
    "                _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
    "\n",
    "        assert len(nbest) >= 1\n",
    "\n",
    "        total_scores = []\n",
    "        for entry in nbest:\n",
    "            total_scores.append(entry.start_logit + entry.end_logit)\n",
    "\n",
    "        probs = _compute_softmax(total_scores)\n",
    "\n",
    "        nbest_json = []\n",
    "        for (i, entry) in enumerate(nbest):\n",
    "            output = collections.OrderedDict()\n",
    "            output[\"text\"] = entry.text\n",
    "            output[\"probability\"] = probs[i]\n",
    "            output[\"start_logit\"] = entry.start_logit\n",
    "            output[\"end_logit\"] = entry.end_logit\n",
    "            nbest_json.append(output)\n",
    "\n",
    "        assert len(nbest_json) >= 1\n",
    "\n",
    "        return nbest_json[0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqdHWTavwBTQ",
    "colab_type": "text"
   },
   "source": [
    "### 6.2 get_final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "BFCUK-W0wBdG",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n",
    "    \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n",
    "\n",
    "    # When we created the data, we kept track of the alignment between original\n",
    "    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n",
    "    # now `orig_text` contains the span of our original text corresponding to the\n",
    "    # span that we predicted.\n",
    "    #\n",
    "    # However, `orig_text` may contain extra characters that we don't want in\n",
    "    # our prediction.\n",
    "    #\n",
    "    # For example, let's say:\n",
    "    #   pred_text = steve smith\n",
    "    #   orig_text = Steve Smith's\n",
    "    #\n",
    "    # We don't want to return `orig_text` because it contains the extra \"'s\".\n",
    "    #\n",
    "    # We don't want to return `pred_text` because it's already been normalized\n",
    "    # (the SQuAD eval script also does punctuation stripping/lower casing but\n",
    "    # our tokenizer does additional normalization like stripping accent\n",
    "    # characters).\n",
    "    #\n",
    "    # What we really want to return is \"Steve Smith\".\n",
    "    #\n",
    "    # Therefore, we have to apply a semi-complicated alignment heruistic between\n",
    "    # `pred_text` and `orig_text` to get a character-to-charcter alignment. This\n",
    "    # can fail in certain cases in which case we just return `orig_text`.\n",
    "\n",
    "    def _strip_spaces(text):\n",
    "        ns_chars = []\n",
    "        ns_to_s_map = collections.OrderedDict()\n",
    "        for (i, c) in enumerate(text):\n",
    "            if c == \" \":\n",
    "                continue\n",
    "            ns_to_s_map[len(ns_chars)] = i\n",
    "            ns_chars.append(c)\n",
    "        ns_text = \"\".join(ns_chars)\n",
    "        return (ns_text, ns_to_s_map)\n",
    "\n",
    "    # We first tokenize `orig_text`, strip whitespace from the result\n",
    "    # and `pred_text`, and check if they are the same length. If they are\n",
    "    # NOT the same length, the heuristic has failed. If they are the same\n",
    "    # length, we assume the characters are one-to-one aligned.\n",
    "    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
    "\n",
    "    tok_text = \" \".join(tokenizer.tokenize(orig_text))\n",
    "\n",
    "    start_position = tok_text.find(pred_text)\n",
    "    if start_position == -1:\n",
    "        if verbose_logging:\n",
    "            logger.info(\n",
    "                \"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n",
    "        return orig_text\n",
    "    end_position = start_position + len(pred_text) - 1\n",
    "\n",
    "    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n",
    "    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n",
    "\n",
    "    if len(orig_ns_text) != len(tok_ns_text):\n",
    "        if verbose_logging:\n",
    "            logger.info(\"Length not equal after stripping spaces: '%s' vs '%s'\",\n",
    "                            orig_ns_text, tok_ns_text)\n",
    "        return orig_text\n",
    "\n",
    "    # We then project the characters in `pred_text` back to `orig_text` using\n",
    "    # the character-to-character alignment.\n",
    "    tok_s_to_ns_map = {}\n",
    "    for (i, tok_index) in six.iteritems(tok_ns_to_s_map):\n",
    "        tok_s_to_ns_map[tok_index] = i\n",
    "\n",
    "    orig_start_position = None\n",
    "    if start_position in tok_s_to_ns_map:\n",
    "        ns_start_position = tok_s_to_ns_map[start_position]\n",
    "        if ns_start_position in orig_ns_to_s_map:\n",
    "            orig_start_position = orig_ns_to_s_map[ns_start_position]\n",
    "\n",
    "    if orig_start_position is None:\n",
    "        if verbose_logging:\n",
    "            logger.info(\"Couldn't map start position\")\n",
    "        return orig_text\n",
    "\n",
    "    orig_end_position = None\n",
    "    if end_position in tok_s_to_ns_map:\n",
    "        ns_end_position = tok_s_to_ns_map[end_position]\n",
    "        if ns_end_position in orig_ns_to_s_map:\n",
    "            orig_end_position = orig_ns_to_s_map[ns_end_position]\n",
    "\n",
    "    if orig_end_position is None:\n",
    "        if verbose_logging:\n",
    "            logger.info(\"Couldn't map end position\")\n",
    "        return orig_text\n",
    "\n",
    "    output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hPk4byxtKAG",
    "colab_type": "text"
   },
   "source": [
    "### 6.3 parse_input_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "i-wvzICgVwRs",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def parse_input_examples(qas_id, paragraph_text, question_text):\n",
    "    \n",
    "    def is_whitespace(c):\n",
    "        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    examples = []\n",
    "\n",
    "    doc_tokens = []\n",
    "    char_to_word_offset = []\n",
    "    prev_is_whitespace = True\n",
    "    for c in paragraph_text:\n",
    "        if is_whitespace(c):\n",
    "            prev_is_whitespace = True\n",
    "        else:\n",
    "            if prev_is_whitespace:\n",
    "                doc_tokens.append(c)\n",
    "            else:\n",
    "                doc_tokens[-1] += c\n",
    "            prev_is_whitespace = False\n",
    "    start_position = None\n",
    "    end_position = None\n",
    "    orig_answer_text = None\n",
    "                \n",
    "    example = SquadExample(\n",
    "        qas_id=qas_id,\n",
    "        question_text=question_text,\n",
    "        doc_tokens=doc_tokens,\n",
    "        orig_answer_text=orig_answer_text,\n",
    "        start_position=start_position,\n",
    "        end_position=end_position)\n",
    "    examples.append(example)\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "XTMhBvybEOYl",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "BERT_CONFIG={\n",
    "    'bert_config_file' : 'bert_config.json',\n",
    "    'vocab_file' : 'vocab.txt',\n",
    "    'output_dir' : 'output',\n",
    "    'train_file' : 'train.json',\n",
    "    'predict_file' : 'dev.json',\n",
    "    'init_checkpoint': None, ## checkpint 파일명 or None\n",
    "    'do_lower_case' : False, \n",
    "    'max_seq_length' : 384,\n",
    "    'doc_stride' : 128,\n",
    "    'max_query_length' : 64,\n",
    "    'do_train' : False,\n",
    "    'do_predict' : False,\n",
    "    'do_chat' : True,\n",
    "    'train_batch_size' : 32,\n",
    "    'predict_batch_size' : 8,\n",
    "    'learning_rate' : 5e-5,\n",
    "    'num_train_epochs' : 3.0,\n",
    "    'warmup_proportion' : 0.1,\n",
    "    'save_checkpoints_steps' : 1000,\n",
    "    'iterations_per_loop' : 1000,\n",
    "    'n_best_size' : 20,\n",
    "    'max_answer_length' : 30,\n",
    "    'verbose_logging' : False,\n",
    "    'no_cuda' : False,\n",
    "    'local_rank' : -1,\n",
    "    'accumulate_gradients' : 1,\n",
    "    'seed' : 42,\n",
    "    'gradient_accumulation_steps' : 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTp-2IXWr9tR",
    "colab_type": "text"
   },
   "source": [
    "##7.main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "utvdkhbmkhJO",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "args = Namespace(**BERT_CONFIG)\n",
    "print(args)\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "logger.info(\"device %s n_gpu %d distributed training %r\", device, n_gpu, bool(args.local_rank != -1))\n",
    "if args.accumulate_gradients < 1:\n",
    "    raise ValueError(\"Invalid accumulate_gradients parameter: {}, should be >= 1\".format(\n",
    "        args.accumulate_gradients))\n",
    "\n",
    "args.train_batch_size = int(args.train_batch_size / args.accumulate_gradients)\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "if args.do_train:\n",
    "    if not args.train_file:\n",
    "        raise ValueError(\n",
    "            \"If `do_train` is True, then `train_file` must be specified.\")\n",
    "if args.do_predict:\n",
    "    if not args.predict_file:\n",
    "        raise ValueError(\n",
    "            \"If `do_predict` is True, then `predict_file` must be specified.\")\n",
    "\n",
    "bert_config = BertConfig.from_json_file(args.bert_config_file)\n",
    "\n",
    "if args.max_seq_length > bert_config.max_position_embeddings:\n",
    "    raise ValueError(\n",
    "        \"Cannot use sequence length %d because the BERT model \"\n",
    "        \"was only trained up to sequence length %d\" %\n",
    "        (args.max_seq_length, bert_config.max_position_embeddings))\n",
    "\n",
    "if os.path.exists(args.output_dir) and os.listdir(args.output_dir):\n",
    "    raise ValueError(\"Output directory () already exists and is not empty.\")\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "tokenizer = FullTokenizer(\n",
    "    vocab_file=args.vocab_file, do_lower_case=args.do_lower_case)\n",
    "\n",
    "train_examples = None\n",
    "num_train_steps = None\n",
    "if args.do_train:\n",
    "    train_examples = read_squad_examples(\n",
    "        input_file=args.train_file, is_training=True)\n",
    "    num_train_steps = int(\n",
    "        len(train_examples) / args.train_batch_size * args.num_train_epochs)\n",
    "\n",
    "model = BertForQuestionAnswering(bert_config)\n",
    "if args.init_checkpoint is not None:\n",
    "    model.load_state_dict(torch.load(args.init_checkpoint, map_location='cpu'))\n",
    "model.to(device)\n",
    "\n",
    "if args.local_rank != -1:\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
    "                                                      output_device=args.local_rank)\n",
    "elif n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if n not in no_decay], 'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if n in no_decay], 'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = BERTAdam(optimizer_parameters,\n",
    "                     lr=args.learning_rate,\n",
    "                     warmup=args.warmup_proportion,\n",
    "                     t_total=num_train_steps)\n",
    "\n",
    "global_step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQvxBmpNsfHz",
    "colab_type": "text"
   },
   "source": [
    "### 7.1train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "K8haOibhsHhT",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "if args.do_train:\n",
    "    train_features = convert_examples_to_features(\n",
    "        examples=train_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=args.max_seq_length,\n",
    "        doc_stride=args.doc_stride,\n",
    "        max_query_length=args.max_query_length,\n",
    "        is_training=True)\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num orig examples = %d\", len(train_examples))\n",
    "    logger.info(\"  Num split examples = %d\", len(train_features))\n",
    "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "    logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "    all_start_positions = torch.tensor([f.start_position for f in train_features], dtype=torch.long)\n",
    "    all_end_positions = torch.tensor([f.end_position for f in train_features], dtype=torch.long)\n",
    "\n",
    "    train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                               all_start_positions, all_end_positions)\n",
    "    if args.local_rank == -1:\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "    else:\n",
    "        train_sampler = DistributedSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "            input_ids, input_mask, segment_ids, start_positions, end_positions = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            start_positions = start_positions.to(device)\n",
    "            end_positions = start_positions.to(device)\n",
    "\n",
    "            start_positions = start_positions.view(-1, 1)\n",
    "            end_positions = end_positions.view(-1, 1)\n",
    "\n",
    "            loss, _ = model(input_ids, segment_ids, input_mask, start_positions, end_positions)\n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu.\n",
    "\n",
    "            loss.backward()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()    # We have accumulated enought gradients\n",
    "                model.zero_grad()\n",
    "                global_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxA9FcoWsmSV",
    "colab_type": "text"
   },
   "source": [
    "### 7.2predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "a69tIqhvsHzr",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "if args.do_predict:\n",
    "    eval_examples = read_squad_examples(\n",
    "        input_file=args.predict_file, is_training=False)\n",
    "    eval_features = convert_examples_to_features(\n",
    "        examples=eval_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=args.max_seq_length,\n",
    "        doc_stride=args.doc_stride,\n",
    "        max_query_length=args.max_query_length,\n",
    "        is_training=False)\n",
    "\n",
    "    logger.info(\"***** Running predictions *****\")\n",
    "    logger.info(\"  Num orig examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Num split examples = %d\", len(eval_features))\n",
    "    logger.info(\"  Batch size = %d\", args.predict_batch_size)\n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "    all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "\n",
    "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index)\n",
    "    if args.local_rank == -1:\n",
    "        eval_sampler = SequentialSampler(eval_data)\n",
    "    else:\n",
    "        eval_sampler = DistributedSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.predict_batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    all_results = []\n",
    "    logger.info(\"Start evaluating\")\n",
    "    for input_ids, input_mask, segment_ids, example_indices in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        if len(all_results) % 1000 == 0:\n",
    "            logger.info(\"Processing example: %d\" % (len(all_results)))\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_start_logits, batch_end_logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "        for i, example_index in enumerate(example_indices):\n",
    "            start_logits = batch_start_logits[i].detach().cpu().tolist()\n",
    "            end_logits = batch_end_logits[i].detach().cpu().tolist()\n",
    "\n",
    "            eval_feature = eval_features[example_index.item()]\n",
    "            unique_id = int(eval_feature.unique_id)\n",
    "            all_results.append(RawResult(unique_id=unique_id,\n",
    "                                         start_logits=start_logits,\n",
    "                                         end_logits=end_logits))\n",
    "\n",
    "    output_prediction_file = os.path.join(args.output_dir, \"predictions.json\")\n",
    "    output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions.json\")\n",
    "    write_predictions(eval_examples, eval_features, all_results,\n",
    "                      args.n_best_size, args.max_answer_length,\n",
    "                      args.do_lower_case, output_prediction_file,\n",
    "                      output_nbest_file, args.verbose_logging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hF-8EhS2sou7",
    "colab_type": "text"
   },
   "source": [
    "### 7.3chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMYWxHqZePTz",
    "colab_type": "text"
   },
   "source": [
    "#### 7.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "7oCps-cNePo0",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "if args.do_chat:\n",
    "    logger.setLevel('CRITICAL')\n",
    "    input_text = None\n",
    "    qas_id = '56be4db0acb8001400a502ed'\n",
    "    c = ContextFinder()\n",
    "    c.build_model(args.predict_file)\n",
    "    print('READY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "3E7fIUt2jhDV",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "    question_text = input('QUESTION: ')\n",
    "    question_text = ' '.join(c.convert_to_lemma(question_text))\n",
    "    context_text = \"\"\n",
    "    for i in c.get_ntop_context_by_cosine_similarity(question_text, 5):\n",
    "        context = ' '.join(c.convert_to_lemma(i))\n",
    "        print(context)\n",
    "        context_text += context + ' '\n",
    "    print('context_result:', context_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzI_KUrBeSqF",
    "colab_type": "text"
   },
   "source": [
    "#### 7.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "sN8BXBirsH4C",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "        eval_examples = parse_input_examples(qas_id, context_text, question_text)\n",
    "        eval_features = convert_examples_to_features(\n",
    "            examples=eval_examples,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=args.max_seq_length,\n",
    "            doc_stride=args.doc_stride,\n",
    "            max_query_length=args.max_query_length,\n",
    "            is_training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWoUzIKHjVRv",
    "colab_type": "text"
   },
   "source": [
    "#### 7.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "onpRlMyljVa1",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "        all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index)\n",
    "        # Run prediction for full data\n",
    "        eval_sampler = SequentialSampler(eval_data)\n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.predict_batch_size)\n",
    "\n",
    "        model.eval()\n",
    "        all_results = []\n",
    "        \n",
    "        for input_ids, input_mask, segment_ids, example_indices in tqdm(eval_dataloader, desc=\"CHAT_INFERENCE\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            with torch.no_grad():\n",
    "                batch_start_logits, batch_end_logits = model(input_ids, segment_ids, input_mask)\n",
    "            for i, example_index in enumerate(example_indices):\n",
    "                start_logits = batch_start_logits[i].detach().cpu().tolist()\n",
    "                end_logits = batch_end_logits[i].detach().cpu().tolist()\n",
    "                eval_feature = eval_features[example_index.item()]\n",
    "                unique_id = int(eval_feature.unique_id)\n",
    "                all_results.append(RawResult(unique_id=unique_id,\n",
    "                                             start_logits=start_logits,\n",
    "                                             end_logits=end_logits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zp4e4yb4zQMr",
    "colab_type": "text"
   },
   "source": [
    "#### 7.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "iiz-hr5yzQUI",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "        answer = chatbot_prediction_answer(eval_examples, eval_features, all_results,\n",
    "                                           args.n_best_size, args.max_answer_length,True)\n",
    "\n",
    "        print('QUESTION:', question_text)\n",
    "        print('ANSWER:', answer)\n",
    "        print('')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "AI-Academy-bert_chat.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
